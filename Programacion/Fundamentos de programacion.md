# 	Fundamentos de programación

En este ebook

## Conceptos básicos de informática

### Informática y computadoras 

### Informática 

Ciencia de computación, conjuntos de conocimientos de la científicos y técnicas que hacen posible el tratamiento automático de la información  por medio de ordenadores

### Computadora

Máquina electrónica, analógica o digital dotada de un memoria de gran capacidad y de métodos de tratamiento de la información, capaz de resolver problemas matemáticos y lógicos mediante la ejecución de programas informáticos

### Hardware

Componentes que integran material de una computadora 

### Software

Programas o instrucciones y reglas informáticas para ejecutar tareas en un programa

Programar 



## temas de hablar cuando se inicia con los fundamentos de la programación conseptos generales 

- las variables 
- como declarar una variable 
- scope de las variables 
- tipos de datos
- sobre que son las cadenas y como se representa en la programación 

Si nos centramos mas en los números, nuestros sistema de numeración esta compuesto por 10 el decimal, expliquemos mas a detallé, nuestro sistema de numeración se formo a partir de la cantidad de los dedos que poseemos en la mano, que pasa cuando se nos agota después del 10, pues continuamos con un 1 después de 10 este seria 11 y así sucesivamente hasta el 99 y volvemos con el 100 y al infinito, en cambio las computadoras su sistema de numeración es base dos 0 1 y 

Nuestro  sistema está compuesto por 10 dígitos diferentes. Una vez que hemos  agotado nuestros dígitos disponibles, representamos números más grandes  usando 2 (luego 3, 4, 5,…) dígitos colocados uno al lado del otro. Por  ejemplo, el número después del 9 es 10, el número después del 99 es 100 y así sucesivamente. Las computadoras hacen lo mismo, pero solo tienen 2  dígitos en lugar de 10. Así que el conteo se ve así: 0, 1, 10, 11, 100,  101, 110, 111 y así sucesivamente. La otra diferencia entre el sistema  numérico que usamos y el que usan las computadoras es que todos los  tipos de números enteros tienen un tamaño definido. Solo tienen espacio  para una cierta cantidad de dígitos. Entonces, un entero de 4 bits  podría verse así: 0000, 0001, 0010, 0011, 0100. Al final, nos quedamos  sin espacio y la mayoría de las computadoras simplemente comienzan al  principio. (Lo que puede resultar en un comportamiento muy extraño)